{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIwaTtuSINOOOVnVv9iUNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/independentmisfit/Speechsense/blob/main/Speechsenseipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDOMJCFyPsue"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import whisper\n",
        "import numpy as np\n",
        "from langdetect import detect\n",
        "from gtts import gTTS\n",
        "from transformers import pipeline\n",
        "from deep_translator import GoogleTranslator  # Updated import for translation\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " Function to map sentiment to emojis\n",
        "def get_sentiment_emoji(sentiment):\n",
        "    emoji_mapping = {\n",
        "        \"disappointment\": \"ğŸ˜\",\n",
        "        \"sadness\": \"ğŸ˜¢\",\n",
        "        \"annoyance\": \"ğŸ˜ \",\n",
        "        \"neutral\": \"ğŸ˜\",\n",
        "        \"disapproval\": \"ğŸ‘\",\n",
        "        \"realization\": \"ğŸ˜®\",\n",
        "        \"nervousness\": \"ğŸ˜¬\",\n",
        "        \"approval\": \"ğŸ‘\",\n",
        "        \"joy\": \"ğŸ˜„\",\n",
        "        \"anger\": \"ğŸ˜¡\",\n",
        "        \"embarrassment\": \"ğŸ˜³\",\n",
        "        \"caring\": \"ğŸ¤—\",\n",
        "        \"remorse\": \"ğŸ˜”\",\n",
        "        \"disgust\": \"ğŸ¤¢\",\n",
        "        \"grief\": \"ğŸ˜¥\",\n",
        "        \"confusion\": \"ğŸ˜•\",\n",
        "        \"relief\": \"ğŸ˜Œ\",\n",
        "        \"desire\": \"ğŸ˜\",\n",
        "        \"admiration\": \"ğŸ˜Œ\",\n",
        "        \"optimism\": \"ğŸ˜Š\",\n",
        "        \"fear\": \"ğŸ˜¨\",\n",
        "        \"love\": \"â¤ï¸\",\n",
        "        \"excitement\": \"ğŸ‰\",\n",
        "        \"curiosity\": \"ğŸ¤”\",\n",
        "        \"amusement\": \"ğŸ˜„\",\n",
        "        \"surprise\": \"ğŸ˜²\",\n",
        "        \"gratitude\": \"ğŸ™\",\n",
        "        \"pride\": \"ğŸ¦\"\n",
        "    }\n",
        "    return emoji_mapping.get(sentiment, \"\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "uLkR786sP6JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "title = \"\"\"# ğŸ™ï¸ SpeechSense: Transcription, Emotion Detection, Translation & Voice Playback ğŸŒ"
      ],
      "metadata": {
        "id": "ZXIZQD1UP8xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description for the app\n",
        "description = description = \"\"\"App Description:\n",
        "SpeechSense uses cutting-edge technologies to process audio efficiently. It employs OpenAIâ€™s Whisper model for accurate transcription, DistilBERT for emotion detection, LangDetect for language identification, Google Translate API for multilingual translation, and gTTS for natural text-to-speech conversion. Together, these models enable real-time speech-to-text, emotion recognition, translation, and voice playback in multiple languages., and then it works its magic by:\n",
        "-Transcribing your speech into text ğŸ“.\n",
        "-Detecting the language of your spoken words ğŸŒ.\n",
        "-Recognizing emotions in your voice, like happiness, sadness, or excitement ğŸ˜„ğŸ˜¢ğŸ˜ .\n",
        "-Translating the transcribed text into a language of your choice ğŸ¯.\n",
        "-Playing back the translated text as audio using natural-sounding voices ğŸ”Š.\n",
        "-Whether youâ€™re looking to translate a conversation, identify emotions in speech, or just hear your words in another language, SpeechSense simplifies it all for you!\n",
        "How to Use the App:\n",
        "Step-by-Step Guide:\n",
        "-Choose Audio Input Type ğŸ¤:\n",
        "  -Select how you want to provide audio:\n",
        "    -Record Audio: Speak directly into your microphone.\n",
        "    -Upload Audio: Upload an existing audio file (MP3 or WAV format).\n",
        "-Record or Upload Your Audio ğŸ§:\n",
        "    -If recording, click the record button and speak.\n",
        "    -If uploading, click to browse your device and select an audio file.\n",
        "-Select a Language for Translation ğŸŒ:\n",
        "    -From the dropdown, choose the language you want your speech to be translated into. For example, you can select Spanish, French, Chinese, or any other language!\n",
        "-Process the Audio ğŸ–±ï¸:\n",
        "  -Click the Process Audio button. The app will:\n",
        "  -Transcribe your audio into text.\n",
        "  -Detect the language you spoke.\n",
        "  -Analyze and identify any emotions in the speech.\n",
        "  -Translate the transcription into your chosen language.\n",
        "  -Convert the translated text into speech and play it back!\n",
        "-Review and Listen ğŸ¶:\n",
        "  -Youâ€™ll see:\n",
        "    -Transcribed Text: The text version of your speech.\n",
        "    -Detected Language: The language spoken in the audio.\n",
        "    -Recognized Emotion: The dominant emotion in your speech.\n",
        "    -Translated Text: The text translated into your selected language.\n",
        "    -Audio Output: Listen to the translation read aloud!\n",
        "-Key Features:\n",
        "-Dual Input: Record audio or upload an audio file for transcription and processing.\n",
        "-Speech-to-Text: Accurately converts speech into written text.\n",
        "-Language Identification: Detects the language of the spoken words.\n",
        "-Emotion Detection: Recognizes emotions such as joy, sadness, and more.\n",
        "-Multilingual Translation: Supports translation into numerous languages.\n",
        "-Text-to-Speech (TTS): Reads out the translated text in a natural-sounding voice.\n",
        "-SpeechSense is your go-to tool for real-time speech transcription, emotion detection, multilingual translation, and audio playbackâ€”all in one place!\"\"\"\n"
      ],
      "metadata": {
        "id": "2wUJ3NUxQDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", framework=\"pt\", model=\"SamLowe/roberta-base-go_emotions\")\n",
        "translator_instance = GoogleTranslator()\n",
        "language_options = translator_instance.get_supported_languages(as_dict=True)\n",
        "language_display = {name.title(): code for name, code in language_options.items()}\n",
        "\n",
        "\n",
        "# Function to process audio using Whisper and deep-translator\n",
        "def process_audio(audio, selected_language_name):\n",
        "    target_language = language_display[selected_language_name]\n",
        "\n",
        "    # Check if file was uploaded or recorded\n",
        "    if isinstance(audio, dict):\n",
        "        audio_file_path = audio['name']  # Get file path of uploaded file\n",
        "    else:\n",
        "        audio_file_path = audio  # Handle recorded audio case\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = whisper_model.transcribe(audio_file_path)\n",
        "    text = result['text']\n",
        "\n",
        "    # Detect the language of the text\n",
        "    detected_language = detect(text)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiment = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = sentiment['label']\n",
        "    sentiment_emoji = get_sentiment_emoji(sentiment_label)\n",
        "\n",
        "    # Translate the text using Deep Translator\n",
        "    translated_text = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
        "    # Language options for translation\n",
        "    language_options = GoogleTranslator.get_supported_languages(as_dict=True)\n",
        "    language_display = {name.title(): code for code, name in language_options.items()}\n",
        "\n",
        "\n",
        "\n",
        "    # Convert the translated text to speech\n",
        "    tts = gTTS(translated_text, lang=target_language)\n",
        "    tts_file = \"output_audio.mp3\"\n",
        "    tts.save(tts_file)\n",
        "\n",
        "    return text, detected_language, sentiment_emoji, sentiment['score'], translated_text, tts_file\n"
      ],
      "metadata": {
        "id": "OMk__QTRQbTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as interface:\n",
        "    gr.Markdown(title)\n",
        "    gr.Markdown(description)\n",
        "\n",
        "    # Select whether to record or upload audio\n",
        "    audio_input_type = gr.Radio([\"Record Audio\", \"Upload Audio\"], label=\"Choose Audio Input Type\", value=\"Record Audio\")\n",
        "\n",
        "    # Set up recording and upload input options\n",
        "    with gr.Row():\n",
        "        audio_record = gr.Audio(label=\"Record your audio\", type=\"filepath\", visible=True, sources=\"microphone\")\n",
        "        audio_upload = gr.File(type=\"filepath\", label=\"Upload an audio file (MP3/WAV)\", file_types=[\"audio/mp3\", \"audio/wav\"], visible=False)\n",
        "\n",
        "\n",
        "    # Language selection\n",
        "    target_language = gr.Dropdown(choices=list(language_display.keys()), value=\"English\", label=\"Select language for translation\", interactive=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Process button\n",
        "    submit_button = gr.Button(\"Process Audio\")\n",
        "\n",
        "    # Outputs for transcription, language, sentiment, translation, and audio\n",
        "    transcription_output = gr.Textbox(label=\"Transcribed Text\")\n",
        "    detected_language_output = gr.Textbox(label=\"Detected Language\")\n",
        "    sentiment_output = gr.Textbox(label=\"Sentiment\")\n",
        "    sentiment_score_output = gr.Textbox(label=\"Sentiment Score\")\n",
        "    translated_text_output = gr.Textbox(label=\"Translated Text\")\n",
        "    audio_output = gr.Audio(label=\"Translated Audio\")\n",
        "\n",
        "    # Function to toggle between audio inputs\n",
        "    def update_audio_input(input_type):\n",
        "        return gr.update(visible=input_type == \"Record Audio\"), gr.update(visible=input_type == \"Upload Audio\")\n",
        "\n",
        "    # Handle audio input processing\n",
        "    def process_based_on_input(input_type, record_audio, upload_audio, selected_language):\n",
        "        if input_type == \"Record Audio\" and record_audio is not None:\n",
        "            audio_file = record_audio\n",
        "        elif input_type == \"Upload Audio\" and upload_audio is not None:\n",
        "            audio_file = upload_audio.name\n",
        "        else:\n",
        "            return \"No audio file provided.\", None, None, None, None\n",
        "\n",
        "        # Process the audio and return the outputs\n",
        "        return process_audio(audio_file, selected_language)\n",
        "\n",
        "    # Link the inputs to the function\n",
        "    audio_input_type.change(fn=update_audio_input, inputs=audio_input_type, outputs=[audio_record, audio_upload])\n",
        "    submit_button.click(fn=process_based_on_input, inputs=[audio_input_type, audio_record, audio_upload, target_language], outputs=[transcription_output, detected_language_output, sentiment_output, sentiment_score_output, translated_text_output, audio_output])\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "UTcYP9T3QidO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}