{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIwaTtuSINOOOVnVv9iUNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/independentmisfit/Speechsense/blob/main/Speechsenseipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDOMJCFyPsue"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import whisper\n",
        "import numpy as np\n",
        "from langdetect import detect\n",
        "from gtts import gTTS\n",
        "from transformers import pipeline\n",
        "from deep_translator import GoogleTranslator  # Updated import for translation\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " Function to map sentiment to emojis\n",
        "def get_sentiment_emoji(sentiment):\n",
        "    emoji_mapping = {\n",
        "        \"disappointment\": \"😞\",\n",
        "        \"sadness\": \"😢\",\n",
        "        \"annoyance\": \"😠\",\n",
        "        \"neutral\": \"😐\",\n",
        "        \"disapproval\": \"👎\",\n",
        "        \"realization\": \"😮\",\n",
        "        \"nervousness\": \"😬\",\n",
        "        \"approval\": \"👍\",\n",
        "        \"joy\": \"😄\",\n",
        "        \"anger\": \"😡\",\n",
        "        \"embarrassment\": \"😳\",\n",
        "        \"caring\": \"🤗\",\n",
        "        \"remorse\": \"😔\",\n",
        "        \"disgust\": \"🤢\",\n",
        "        \"grief\": \"😥\",\n",
        "        \"confusion\": \"😕\",\n",
        "        \"relief\": \"😌\",\n",
        "        \"desire\": \"😍\",\n",
        "        \"admiration\": \"😌\",\n",
        "        \"optimism\": \"😊\",\n",
        "        \"fear\": \"😨\",\n",
        "        \"love\": \"❤️\",\n",
        "        \"excitement\": \"🎉\",\n",
        "        \"curiosity\": \"🤔\",\n",
        "        \"amusement\": \"😄\",\n",
        "        \"surprise\": \"😲\",\n",
        "        \"gratitude\": \"🙏\",\n",
        "        \"pride\": \"🦁\"\n",
        "    }\n",
        "    return emoji_mapping.get(sentiment, \"\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "uLkR786sP6JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "title = \"\"\"# 🎙️ SpeechSense: Transcription, Emotion Detection, Translation & Voice Playback 🌍"
      ],
      "metadata": {
        "id": "ZXIZQD1UP8xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description for the app\n",
        "description = description = \"\"\"App Description:\n",
        "SpeechSense uses cutting-edge technologies to process audio efficiently. It employs OpenAI’s Whisper model for accurate transcription, DistilBERT for emotion detection, LangDetect for language identification, Google Translate API for multilingual translation, and gTTS for natural text-to-speech conversion. Together, these models enable real-time speech-to-text, emotion recognition, translation, and voice playback in multiple languages., and then it works its magic by:\n",
        "-Transcribing your speech into text 📝.\n",
        "-Detecting the language of your spoken words 🌐.\n",
        "-Recognizing emotions in your voice, like happiness, sadness, or excitement 😄😢😠.\n",
        "-Translating the transcribed text into a language of your choice 🎯.\n",
        "-Playing back the translated text as audio using natural-sounding voices 🔊.\n",
        "-Whether you’re looking to translate a conversation, identify emotions in speech, or just hear your words in another language, SpeechSense simplifies it all for you!\n",
        "How to Use the App:\n",
        "Step-by-Step Guide:\n",
        "-Choose Audio Input Type 🎤:\n",
        "  -Select how you want to provide audio:\n",
        "    -Record Audio: Speak directly into your microphone.\n",
        "    -Upload Audio: Upload an existing audio file (MP3 or WAV format).\n",
        "-Record or Upload Your Audio 🎧:\n",
        "    -If recording, click the record button and speak.\n",
        "    -If uploading, click to browse your device and select an audio file.\n",
        "-Select a Language for Translation 🌍:\n",
        "    -From the dropdown, choose the language you want your speech to be translated into. For example, you can select Spanish, French, Chinese, or any other language!\n",
        "-Process the Audio 🖱️:\n",
        "  -Click the Process Audio button. The app will:\n",
        "  -Transcribe your audio into text.\n",
        "  -Detect the language you spoke.\n",
        "  -Analyze and identify any emotions in the speech.\n",
        "  -Translate the transcription into your chosen language.\n",
        "  -Convert the translated text into speech and play it back!\n",
        "-Review and Listen 🎶:\n",
        "  -You’ll see:\n",
        "    -Transcribed Text: The text version of your speech.\n",
        "    -Detected Language: The language spoken in the audio.\n",
        "    -Recognized Emotion: The dominant emotion in your speech.\n",
        "    -Translated Text: The text translated into your selected language.\n",
        "    -Audio Output: Listen to the translation read aloud!\n",
        "-Key Features:\n",
        "-Dual Input: Record audio or upload an audio file for transcription and processing.\n",
        "-Speech-to-Text: Accurately converts speech into written text.\n",
        "-Language Identification: Detects the language of the spoken words.\n",
        "-Emotion Detection: Recognizes emotions such as joy, sadness, and more.\n",
        "-Multilingual Translation: Supports translation into numerous languages.\n",
        "-Text-to-Speech (TTS): Reads out the translated text in a natural-sounding voice.\n",
        "-SpeechSense is your go-to tool for real-time speech transcription, emotion detection, multilingual translation, and audio playback—all in one place!\"\"\"\n"
      ],
      "metadata": {
        "id": "2wUJ3NUxQDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", framework=\"pt\", model=\"SamLowe/roberta-base-go_emotions\")\n",
        "translator_instance = GoogleTranslator()\n",
        "language_options = translator_instance.get_supported_languages(as_dict=True)\n",
        "language_display = {name.title(): code for name, code in language_options.items()}\n",
        "\n",
        "\n",
        "# Function to process audio using Whisper and deep-translator\n",
        "def process_audio(audio, selected_language_name):\n",
        "    target_language = language_display[selected_language_name]\n",
        "\n",
        "    # Check if file was uploaded or recorded\n",
        "    if isinstance(audio, dict):\n",
        "        audio_file_path = audio['name']  # Get file path of uploaded file\n",
        "    else:\n",
        "        audio_file_path = audio  # Handle recorded audio case\n",
        "\n",
        "    # Transcribe the audio using Whisper\n",
        "    result = whisper_model.transcribe(audio_file_path)\n",
        "    text = result['text']\n",
        "\n",
        "    # Detect the language of the text\n",
        "    detected_language = detect(text)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiment = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = sentiment['label']\n",
        "    sentiment_emoji = get_sentiment_emoji(sentiment_label)\n",
        "\n",
        "    # Translate the text using Deep Translator\n",
        "    translated_text = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
        "    # Language options for translation\n",
        "    language_options = GoogleTranslator.get_supported_languages(as_dict=True)\n",
        "    language_display = {name.title(): code for code, name in language_options.items()}\n",
        "\n",
        "\n",
        "\n",
        "    # Convert the translated text to speech\n",
        "    tts = gTTS(translated_text, lang=target_language)\n",
        "    tts_file = \"output_audio.mp3\"\n",
        "    tts.save(tts_file)\n",
        "\n",
        "    return text, detected_language, sentiment_emoji, sentiment['score'], translated_text, tts_file\n"
      ],
      "metadata": {
        "id": "OMk__QTRQbTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as interface:\n",
        "    gr.Markdown(title)\n",
        "    gr.Markdown(description)\n",
        "\n",
        "    # Select whether to record or upload audio\n",
        "    audio_input_type = gr.Radio([\"Record Audio\", \"Upload Audio\"], label=\"Choose Audio Input Type\", value=\"Record Audio\")\n",
        "\n",
        "    # Set up recording and upload input options\n",
        "    with gr.Row():\n",
        "        audio_record = gr.Audio(label=\"Record your audio\", type=\"filepath\", visible=True, sources=\"microphone\")\n",
        "        audio_upload = gr.File(type=\"filepath\", label=\"Upload an audio file (MP3/WAV)\", file_types=[\"audio/mp3\", \"audio/wav\"], visible=False)\n",
        "\n",
        "\n",
        "    # Language selection\n",
        "    target_language = gr.Dropdown(choices=list(language_display.keys()), value=\"English\", label=\"Select language for translation\", interactive=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Process button\n",
        "    submit_button = gr.Button(\"Process Audio\")\n",
        "\n",
        "    # Outputs for transcription, language, sentiment, translation, and audio\n",
        "    transcription_output = gr.Textbox(label=\"Transcribed Text\")\n",
        "    detected_language_output = gr.Textbox(label=\"Detected Language\")\n",
        "    sentiment_output = gr.Textbox(label=\"Sentiment\")\n",
        "    sentiment_score_output = gr.Textbox(label=\"Sentiment Score\")\n",
        "    translated_text_output = gr.Textbox(label=\"Translated Text\")\n",
        "    audio_output = gr.Audio(label=\"Translated Audio\")\n",
        "\n",
        "    # Function to toggle between audio inputs\n",
        "    def update_audio_input(input_type):\n",
        "        return gr.update(visible=input_type == \"Record Audio\"), gr.update(visible=input_type == \"Upload Audio\")\n",
        "\n",
        "    # Handle audio input processing\n",
        "    def process_based_on_input(input_type, record_audio, upload_audio, selected_language):\n",
        "        if input_type == \"Record Audio\" and record_audio is not None:\n",
        "            audio_file = record_audio\n",
        "        elif input_type == \"Upload Audio\" and upload_audio is not None:\n",
        "            audio_file = upload_audio.name\n",
        "        else:\n",
        "            return \"No audio file provided.\", None, None, None, None\n",
        "\n",
        "        # Process the audio and return the outputs\n",
        "        return process_audio(audio_file, selected_language)\n",
        "\n",
        "    # Link the inputs to the function\n",
        "    audio_input_type.change(fn=update_audio_input, inputs=audio_input_type, outputs=[audio_record, audio_upload])\n",
        "    submit_button.click(fn=process_based_on_input, inputs=[audio_input_type, audio_record, audio_upload, target_language], outputs=[transcription_output, detected_language_output, sentiment_output, sentiment_score_output, translated_text_output, audio_output])\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "UTcYP9T3QidO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}